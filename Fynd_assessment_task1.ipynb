{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7SlwEbB-iyqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b21323-283e-4847-f030-0599770b0a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai tqdm pandas\n",
        "!pip install --upgrade google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBqWz5_soouh",
        "outputId": "9ae8e41f-7edb-4172-c7c7-ab5cf86f3be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "status: 200\n",
            "{\n",
            "  \"models\": [\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.5-flash\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 2.5 Flash\",\n",
            "      \"description\": \"Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\",\n",
            "        \"batchGenerateContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2,\n",
            "      \"thinking\": true\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.5-pro\",\n",
            "      \"version\": \"2.5\",\n",
            "      \"displayName\": \"Gemini 2.5 Pro\",\n",
            "      \"description\": \"Stable release (June 17th, 2025) of Gemini 2.5 Pro\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\",\n",
            "        \"batchGenerateContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2,\n",
            "      \"thinking\": true\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash\",\n",
            "      \"description\": \"Gemini 2.0 Flash\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\",\n",
            "        \"batchGenerateContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-001\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash 001\",\n",
            "      \"description\": \"Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerat\n"
          ]
        }
      ],
      "source": [
        "import os, requests\n",
        "from google.colab import userdata\n",
        "key = userdata.get('GEMINI_API_KEY') or os.environ.get('GEMINI_API_KEY') or \"AIzaSyCu3yinjDFSLXpwmrjxP3941Tfo1aSyEY4\"\n",
        "url = \"https://generativelanguage.googleapis.com/v1/models\"\n",
        "r = requests.get(url, params={\"key\": key}, timeout=20)\n",
        "print(\"status:\", r.status_code)\n",
        "print(r.text[:2000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9XYNMrtA7qm",
        "outputId": "35754f90-bee7-4f5b-9255-fe7ea63afc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got key in environment/secrets: True\n",
            "HTTP status: 200\n",
            "Body (first 1000 chars):\n",
            " {\n",
            "  \"models\": [\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.5-flash\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 2.5 Flash\",\n",
            "      \"description\": \"Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\",\n",
            "        \"batchGenerateContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2,\n",
            "      \"thinking\": true\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.5-pro\",\n",
            "      \"version\": \"2.5\",\n",
            "      \"displayName\": \"Gemini 2.5 Pro\",\n",
            "      \"description\": \"Stable release (June 17th, 2025) of Gemini 2.5 Pro\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\",\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import os, requests\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('GEMINI_API_KEY') or os.environ.get('GEMINI_API_KEY')\n",
        "print(\"Got key in environment/secrets:\", bool(key))\n",
        "\n",
        "url = \"https://generativelanguage.googleapis.com/v1/models\"\n",
        "r = requests.get(url, params={\"key\": key}, timeout=20)\n",
        "print(\"HTTP status:\", r.status_code)\n",
        "print(\"Body (first 1000 chars):\\n\", r.text[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-BduDMMrH5-I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('GEMINI_API_KEY') or os.environ.get('GEMINI_API_KEY') or \"AIzaSyCu3yinjDFSLXpwmrjxP3941Tfo1aSyEY4\"\n",
        "genai.configure(api_key=key)\n",
        "\n",
        "df = pd.read_csv(\"/content/yelp.csv\")\n",
        "\n",
        "\n",
        "df = df.sample(3, random_state=42).reset_index(drop=True)\n",
        "\n",
        "TEXT = \"text\"\n",
        "STARS = \"stars\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IqYE_ZvdH9i0"
      },
      "outputs": [],
      "source": [
        "def prompt_v1(review):\n",
        "    return f\"\"\"\n",
        "Rate this Yelp review from 1-5.\n",
        "\n",
        "Return ONLY valid JSON:\n",
        "{{\n",
        "  \"predicted_stars\": <1-5>,\n",
        "  \"explanation\": \"<brief reason>\"\n",
        "}}\n",
        "\n",
        "Review: \"{review}\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H80vmqLLICnf"
      },
      "outputs": [],
      "source": [
        "def prompt_v2(review):\n",
        "    return f\"\"\"\n",
        "Analyze this Yelp review step-by-step.\n",
        "\n",
        "1. Identify sentiment.\n",
        "2. Identify positive/negative keywords.\n",
        "3. Decide a star rating (1-5).\n",
        "4. Return ONLY valid JSON.\n",
        "\n",
        "JSON format:\n",
        "{{\n",
        "  \"predicted_stars\": <1-5>,\n",
        "  \"explanation\": \"<why>\"\n",
        "}}\n",
        "\n",
        "Review: \"{review}\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RBza9Dx-IGPp"
      },
      "outputs": [],
      "source": [
        "def prompt_v3(review):\n",
        "    return f\"\"\"\n",
        "You are an expert sentiment classifier.\n",
        "\n",
        "Here are examples:\n",
        "\n",
        "Example 1:\n",
        "Review: \"Terrible service and cold food.\"\n",
        "Output: {{\"predicted_stars\": 1, \"explanation\": \"Very bad experience\"}}\n",
        "\n",
        "Example 2:\n",
        "Review: \"Amazing food! Loved the ambience.\"\n",
        "Output: {{\"predicted_stars\": 5, \"explanation\": \"Highly positive sentiment\"}}\n",
        "\n",
        "Now classify the next review.\n",
        "\n",
        "Return valid JSON only.\n",
        "\n",
        "Review: \"{review}\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def call_llm(prompt):\n",
        "    response = genai.GenerativeModel(\"models/gemini-2.5-flash\").generate_content(prompt)\n",
        "    text = response.text\n",
        "\n",
        "\n",
        "    json_match = re.search(r'```json\\n(.*?)```', text, re.DOTALL)\n",
        "    if json_match:\n",
        "        json_string = json_match.group(1)\n",
        "    else:\n",
        "\n",
        "        json_string = text.strip()\n",
        "\n",
        "    try:\n",
        "        data = json.loads(json_string)\n",
        "        return data, True\n",
        "    except:\n",
        "        return {\"predicted_stars\": None, \"explanation\": \"JSON parse failed\"}, False\n",
        "\n",
        "\n",
        "def evaluate_prompt(df, prompt_function):\n",
        "    predictions = []\n",
        "    json_validity = 0\n",
        "\n",
        "    for r in tqdm(df[TEXT]):\n",
        "        prompt = prompt_function(r)\n",
        "        output, valid = call_llm(prompt)\n",
        "\n",
        "        predictions.append(output[\"predicted_stars\"])\n",
        "        if valid:\n",
        "            json_validity += 1\n",
        "\n",
        "    df[\"predicted\"] = predictions\n",
        "    accuracy = (df[\"predicted\"] == df[STARS]).mean()\n",
        "\n",
        "    return accuracy, json_validity / len(df)"
      ],
      "metadata": {
        "id": "NQA0W0fC6jkX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc1, json1 = evaluate_prompt(df.head(10).copy(), prompt_v1)\n",
        "acc2, json2 = evaluate_prompt(df.head(10).copy(), prompt_v2)\n",
        "acc3, json3 = evaluate_prompt(df.head(10).copy(), prompt_v3)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Prompt Version\": [\"Direct\", \"Chain-of-Thought\", \"Few-Shot\"],\n",
        "    \"Accuracy\": [acc1, acc2, acc3],\n",
        "    \"JSON Validity\": [json1, json2, json3]\n",
        "})\n",
        "\n",
        "print(results.to_json(orient='records', indent=4))"
      ],
      "metadata": {
        "id": "GsDKluyZtxzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SAMPLE_SIZE = 200\n",
        "\n",
        "df_sample = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "clean_outputs = []\n",
        "\n",
        "\n",
        "def get_clean_output(prompt):\n",
        "    \"\"\"Returns clean JSON output parsed from LLM response.\"\"\"\n",
        "    response = genai.GenerativeModel(\"gemini-2.0-flash\").generate_content(prompt)\n",
        "    text = response.text\n",
        "\n",
        "\n",
        "    json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.DOTALL)\n",
        "\n",
        "    if json_match:\n",
        "        json_str = json_match.group(1)\n",
        "    else:\n",
        "\n",
        "        start = text.find(\"{\")\n",
        "        end = text.rfind(\"}\") + 1\n",
        "        json_str = text[start:end] if start != -1 else text\n",
        "\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except:\n",
        "        return {\"predicted_stars\": None, \"explanation\": \"JSON parse failed\"}\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(df_sample)):\n",
        "    review = df_sample.loc[i, \"text\"]\n",
        "    prompt = prompt_v1(review)\n",
        "\n",
        "    result = get_clean_output(prompt)\n",
        "    clean_outputs.append(result)\n",
        "\n",
        "    print(f\"\\n=== Sample {i+1} ===\")\n",
        "    print(json.dumps(result, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "bWINA4ul62J9",
        "outputId": "6843ae75-58a7-47ca-bbff-f43d5ecf244b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sample 1 ===\n",
            "{\n",
            "    \"predicted_stars\": 4,\n",
            "    \"explanation\": \"Positive comments on food, service, cocktails, and patio, with only minor negativity regarding late-night emptiness.\"\n",
            "}\n",
            "\n",
            "=== Sample 2 ===\n",
            "{\n",
            "    \"predicted_stars\": 5,\n",
            "    \"explanation\": \"Extremely positive review referencing authenticity and high quality compared to Louisiana food.\"\n",
            "}\n",
            "\n",
            "=== Sample 3 ===\n",
            "{\n",
            "    \"predicted_stars\": 4,\n",
            "    \"explanation\": \"The reviewer uses positive language like 'good', 'filling', and 'hits the spot', and indicates a regular positive experience ('every friday'). While noting it's 'typical strip mall pizza', the overall sentiment is positive.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SAMPLE_SIZE = 200\n",
        "\n",
        "df_sample = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "clean_outputs = []\n",
        "\n",
        "\n",
        "def get_clean_output(prompt):\n",
        "    \"\"\"Returns clean JSON output parsed from LLM response.\"\"\"\n",
        "    response = genai.GenerativeModel(\"gemini-2.0-flash\").generate_content(prompt)\n",
        "    text = response.text\n",
        "\n",
        "\n",
        "    json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.DOTALL)\n",
        "\n",
        "    if json_match:\n",
        "        json_str = json_match.group(1)\n",
        "    else:\n",
        "\n",
        "        start = text.find(\"{\")\n",
        "        end = text.rfind(\"}\") + 1\n",
        "        json_str = text[start:end] if start != -1 else text\n",
        "\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except:\n",
        "        return {\"predicted_stars\": None, \"explanation\": \"JSON parse failed\"}\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(df_sample)):\n",
        "    review = df_sample.loc[i, \"text\"]\n",
        "    prompt = prompt_v2(review)\n",
        "\n",
        "    result = get_clean_output(prompt)\n",
        "    clean_outputs.append(result)\n",
        "\n",
        "    print(f\"\\n=== Sample {i+1} ===\")\n",
        "    print(json.dumps(result, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "lF676DYk7-2B",
        "outputId": "bf79aaea-9925-468c-bd30-ee211102b2d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sample 1 ===\n",
            "{\n",
            "    \"predicted_stars\": 4,\n",
            "    \"explanation\": \"The review expresses mixed feelings. While the reviewer notes the place was empty, they praise the food ('well made pub grub'), service ('friendly'), and cocktails ('quality'). The mention of the atmosphere working for a sports bar is positive. The added update about the patio is also a strong positive. The only real negative is the emptiness late at night, which is understood in the context of the location. Overall, the positive aspects outweigh the negative, suggesting a 4-star rating.\"\n",
            "}\n",
            "\n",
            "=== Sample 2 ===\n",
            "{\n",
            "    \"predicted_stars\": 5,\n",
            "    \"explanation\": \"The review expresses a highly positive sentiment. The phrase \\\"best she's had outside of Louisiana\\\" indicates that the crawfish etouffee is exceptionally good and comparable to authentic Louisiana cuisine, which is high praise.\"\n",
            "}\n",
            "\n",
            "=== Sample 3 ===\n",
            "{\n",
            "    \"predicted_stars\": 4,\n",
            "    \"explanation\": \"The review expresses a positive sentiment. Keywords like \\\"good,\\\" \\\"filling,\\\" and \\\"hits the spot\\\" indicate satisfaction. The repetition of eating there every Friday also suggests a positive experience. While \\\"Typical strip mall pizza\\\" could be interpreted negatively, the overall tone is positive, pointing to a 4-star rating.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SAMPLE_SIZE = 200\n",
        "\n",
        "df_sample = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "clean_outputs = []\n",
        "\n",
        "\n",
        "def get_clean_output(prompt):\n",
        "    \"\"\"Returns clean JSON output parsed from LLM response.\"\"\"\n",
        "    response = genai.GenerativeModel(\"gemini-2.0-flash\").generate_content(prompt)\n",
        "    text = response.text\n",
        "\n",
        "\n",
        "    json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.DOTALL)\n",
        "\n",
        "    if json_match:\n",
        "        json_str = json_match.group(1)\n",
        "    else:\n",
        "\n",
        "        start = text.find(\"{\")\n",
        "        end = text.rfind(\"}\") + 1\n",
        "        json_str = text[start:end] if start != -1 else text\n",
        "\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except:\n",
        "        return {\"predicted_stars\": None, \"explanation\": \"JSON parse failed\"}\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(df_sample)):\n",
        "    review = df_sample.loc[i, \"text\"]\n",
        "    prompt = prompt_v3(review)\n",
        "\n",
        "    result = get_clean_output(prompt)\n",
        "    clean_outputs.append(result)\n",
        "\n",
        "    print(f\"\\n=== Sample {i+1} ===\")\n",
        "    print(json.dumps(result, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "0GgYWr6t8PfK",
        "outputId": "00953951-1a33-49ed-a834-d908e30723bf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sample 1 ===\n",
            "{\n",
            "    \"predicted_stars\": 4,\n",
            "    \"explanation\": \"Mostly positive review highlighting friendly service, good food and cocktails, and a good patio. The only negative is it being empty late at night, but this is explained by the location.\"\n",
            "}\n",
            "\n",
            "=== Sample 2 ===\n",
            "{\n",
            "    \"predicted_stars\": 5,\n",
            "    \"explanation\": \"Extremely positive review, especially considering the comparison to Louisiana crawfish etouffee.\"\n",
            "}\n",
            "\n",
            "=== Sample 3 ===\n",
            "{\n",
            "    \"predicted_stars\": 4,\n",
            "    \"explanation\": \"Generally positive sentiment. The reviewer eats there regularly and enjoys the food and drink. While it's described as 'typical', the overall impression is favorable.\"\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}